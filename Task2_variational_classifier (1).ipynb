{"cells":[{"cell_type":"markdown","metadata":{"id":"Uo116amjLq2n"},"source":["**The original writeup is**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wl7C800fLeWT"},"source":["PennyLane Documentation\n","Variational Quantum Classifier Tutorial"]},{"cell_type":"markdown","metadata":{"id":"Xg1LUck-MDvO"},"source":["https://pennylane.ai/qml/demos/tutorial_variational_classifier/"]},{"cell_type":"markdown","metadata":{"id":"Mz_zAv8XMTC1"},"source":["**Intro**"]},{"cell_type":"markdown","metadata":{"id":"lPx-Nx4ubzU5"},"source":["Variational Quantum Classifiers (VQCs)\n","\n","Variational Quantum Classifiers (VQCs) are a type of hybrid quantum machine learning algorithm that combines the power of quantum computing with the flexibility of classical machine learning algorithms to achieve state-of-the-art performance on a wide range of classification tasks.\n","They are a class of quantum algorithms that use a variational approach to optimize a quantum circuit for a specific machine learning task, and have been shown to be effective in a variety of applications, including image classification, natural language processing, and more."]},{"cell_type":"markdown","metadata":{"id":"aQ0BXP57fFuF"},"source":["picture for the circuit"]},{"cell_type":"markdown","metadata":{"id":"w1XviL20fJrj"},"source":["**How VQCs Works**\n","\n","A VQC consists of two parts: a classical part for pre- and post-processing data, and a quantum part for harnessing the power of quantum mechanics to perform certain calculations more efficiently. The quantum part is composed of a quantum circuit that behaves similarly to a traditional machine learning algorithm and these are done by some steps\n"]},{"cell_type":"markdown","metadata":{"id":"mGyUBxi8fr7L"},"source":["**Step 1: Encode Classical Data into a Quantum State**\n","\n","To encode classical data into a quantum state, we perform certain operations to help us work with the data in quantum circuits. One of the steps is called data embedding, which is the representation of classical data as a quantum state in Hilbert space via a quantum feature map."]},{"cell_type":"markdown","metadata":{"id":"aW2qo3yWkiLU"},"source":["Angle Encoding: One common method is angle encoding, where classical features are mapped to the angles of rotation gates applied to qubits.\n","Example: For a data vector\n","𝑥\n","x with features\n","𝑥\n","1\n",",\n","𝑥\n","2\n",",\n","…\n",",\n","𝑥\n","𝑛\n","x\n","1\n","​\n"," ,x\n","2\n","​\n"," ,…,x\n","n\n","​\n"," , we encode these features into quantum states using rotation gates. If we have a qubit, we apply a rotation\n","𝑅\n","𝑥\n","(\n","𝜃\n",")\n","R\n","x\n","​\n"," (θ) where\n","𝜃\n","=\n","𝑥\n","𝑖\n","⋅\n","scaling factor\n","θ=x\n","i\n","​\n"," ⋅scaling factor. The quantum state\n","∣\n","𝜓\n","⟩\n","∣ψ⟩ is then prepared as:\n","\n","∣\n","𝜓\n","⟩\n","=\n","𝑅\n","𝑥\n","(\n","𝜃\n","1\n",")\n","𝑅\n","𝑦\n","(\n","𝜃\n","2\n",")\n","⋯\n","𝑅\n","𝑧\n","(\n","𝜃\n","𝑛\n",")\n","∣\n","0\n","⟩\n","∣ψ⟩=R\n","x\n","​\n"," (θ\n","1\n","​\n"," )R\n","y\n","​\n"," (θ\n","2\n","​\n"," )⋯R\n","z\n","​\n"," (θ\n","n\n","​\n"," )∣0⟩\n","Here,\n","∣\n","0\n","⟩\n","∣0⟩ is the initial state of the qubit, and\n","𝑅\n","𝑥\n",",\n","𝑅\n","𝑦\n",",\n","𝑅\n","𝑧\n","R\n","x\n","​\n"," ,R\n","y\n","​\n"," ,R\n","z\n","​\n","  are rotation operators applied to the qubit.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"APwzFimnlGhv"},"source":["**Step 2: inputing our data to a Quantum Circuit**\n","\n","After encoding, a parameterized quantum circuit processes the quantum state. The circuit consists of various quantum gates such as Hadamard, CNOT, and rotation gates. The parameters of these gates are optimized during training.\n","Quantum State Evolution: The quantum state evolves according to the unitary transformations defined by the quantum circuit. If\n","𝑈\n","(\n","𝜃\n",")\n","U(θ) represents the unitary operation parameterized by\n","𝜃\n","θ, the state after applying the circuit is:\n","\n","∣\n","𝜓\n","(\n","𝜃\n",")\n","⟩\n","=\n","𝑈\n","(\n","𝜃\n",")\n","∣\n","𝜓\n","⟩\n","∣ψ(θ)⟩=U(θ)∣ψ⟩\n","where\n","∣\n","𝜓\n","⟩\n","∣ψ⟩ is the encoded state."]},{"cell_type":"markdown","metadata":{"id":"Rt8DfL8Dl8ws"},"source":["Step 3: Apply a Parameterized Model\n","\n","The goal of the training phase is to find a set of parameters that maximizes the accuracy of the classification model. In the classifying phase, the encoded quantum state will evolve through the well-trained ansatz W(θ) with optimized variational parameters θ.\n"]},{"cell_type":"markdown","metadata":{"id":"iF-OLqTLnDAp"},"source":["**The Cost Function and Measurement:**\n","\n","The cost function measures how well the quantum classifier performs. It typically involves:\n","\n","Expectation Value:\n","\n","Quantum measurements provide expectation values that are used to estimate probabilities. For a quantum observable\n","𝑂\n","^\n","O\n","^\n"," , the expectation value is given by:\n","⟨\n","𝜓\n","(\n","𝜃\n",")\n","∣\n","𝑂\n","^\n","∣\n","𝜓\n","(\n","𝜃\n",")\n","⟩\n","⟨ψ(θ)∣\n","O\n","^\n"," ∣ψ(θ)⟩\n","This value represents the probability of measuring a particular outcome related to the observable\n","𝑂\n","^\n","O\n","^\n"," ."]},{"cell_type":"markdown","metadata":{"id":"aqd6RL4gnNDq"},"source":["\n","The loss function (or cost function) quantifies the difference between the predicted and true labels. For classification tasks, the cross-entropy loss is often used:\n","𝐿\n","(\n","𝜃\n",")\n","=\n","−\n","∑\n","𝑖\n","𝑦\n","𝑖\n","log\n","⁡\n","(\n","𝑒\n","𝑦\n","^\n","𝑖\n","∑\n","𝑗\n","𝑒\n","𝑦\n","^\n","𝑗\n",")\n","L(θ)=−\n","i\n","∑\n","​\n"," y\n","i\n","​\n"," log(\n","∑\n","j\n","​\n"," e\n","y\n","^\n","​\n","j\n","​\n","e\n","y\n","^\n","​  \n","i\n","​\n"," )\n"]},{"cell_type":"markdown","metadata":{"id":"q_5oC8R3oner"},"source":["where\n","𝑦\n","^\n","𝑖\n","y\n","^\n","​\n","i\n","​\n","  represents the predicted probability for class\n","𝑖\n","i, and\n","𝑦\n","𝑖\n","y\n","i\n","​\n","  is the true label."]},{"cell_type":"markdown","metadata":{"id":"eMm4y7qgpL7c"},"source":["\n"," **Step 4:Parameter Optimization:**\n","\n","The goal is to find the optimal parameters\n","𝜃\n","θ that minimize the cost function. This is typically done using classical optimization techniques.\n","Gradient-Based Optimization: Gradients of the cost function with respect to the parameters\n","𝜃\n","θ are computed to update the parameters. This can be done using methods like gradient descent or more advanced algorithms.\n","\n","Parameter Shift Rule: A common method to compute gradients in quantum circuits is the parameter shift rule. For a parameterized gate\n","𝑈\n","(\n","𝜃\n",")\n","U(θ), the derivative of the cost function\n","𝐿\n","(\n","𝜃\n",")\n","L(θ) with respect to\n","𝜃\n","θ can be estimated as:\n","\n","∂\n","𝐿\n","(\n","𝜃\n",")\n","∂\n","𝜃\n","≈\n","𝐿\n","(\n","𝜃\n","+\n","𝛿\n",")\n","−\n","𝐿\n","(\n","𝜃\n","−\n","𝛿\n",")\n","2\n","𝛿\n","∂θ\n","∂L(θ)\n","​\n"," ≈\n","2δ\n","L(θ+δ)−L(θ−δ)\n","​\n"]},{"cell_type":"markdown","metadata":{"id":"dIkUQ1f6pjX9"},"source":["where\n","𝛿\n","δ is a small shift value.\n","\n","**Training:**\n","\n","The optimization algorithm iteratively adjusts the parameters to minimize the cost function. This process continues until convergence."]},{"cell_type":"markdown","metadata":{"id":"deLiHFTVDIvp"},"source":["**Now we will do these steps using VQCS with a real world data**"]},{"cell_type":"markdown","metadata":{"id":"TIvlHvU6O6fP"},"source":["In this codebook we will use Iris dataset"]},{"cell_type":"code","source":[],"metadata":{"id":"f94v7BTsSdxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUPKTFlkD72J"},"source":["but frist you need to make sure to upload the data before"]},{"cell_type":"markdown","source":["## 1.Fitting the parity function"],"metadata":{"id":"6G9DkvCkiCd3"}},{"cell_type":"markdown","metadata":{"id":"_f56LFGDPcPt"},"source":["## setup"]},{"cell_type":"markdown","source":[],"metadata":{"id":"-tJAm0Pjf4wu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10067,"status":"ok","timestamp":1723141849047,"user":{"displayName":"menna zaied","userId":"13321966629572407703"},"user_tz":-180},"id":"sSrDfV1-Ps6l","outputId":"df2e46c2-96db-4cb2-ce79-62d2b997e3a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pennylane\n","  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n","Collecting rustworkx (from pennylane)\n","  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n","Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n","Collecting appdirs (from pennylane)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Collecting semantic-version>=2.7 (from pennylane)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting autoray>=0.6.11 (from pennylane)\n","  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.4.0)\n","Collecting pennylane-lightning>=0.37 (from pennylane)\n","  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (1.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.7.4)\n","Downloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: appdirs, semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n","Successfully installed appdirs-1.4.4 autoray-0.6.12 pennylane-0.37.0 pennylane-lightning-0.37.0 rustworkx-0.15.1 semantic-version-2.10.0\n"]}],"source":["pip install pennylane pennylane"]},{"cell_type":"markdown","metadata":{"id":"AX0eqDazcd5W"},"source":["##Imports"]},{"cell_type":"code","source":["import pennylane as qml\n","from pennylane import numpy as np\n","from pennylane.optimize import NesterovMomentumOptimizer"],"metadata":{"id":"eL1Is_RDgIsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8-SSU05kQI8P"},"source":["##Defining the Quantum Circuit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZhQVaIaUFUZD"},"outputs":[],"source":["import pennylane as qml\n","from pennylane import numpy as np\n","from pennylane.optimize import NesterovMomentumOptimizer\n"]},{"cell_type":"markdown","metadata":{"id":"i3fHdB-DdODb"},"source":["##Defining the Variational Quantum Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkPDwc9ZdUhe"},"outputs":[],"source":["def variational_classifier(weights, x):\n","    return circuit(weights, x)"]},{"cell_type":"markdown","metadata":{"id":"cvpggZb9F8fm"},"source":["##Quantum and Classical Nodes\n"]},{"cell_type":"markdown","metadata":{"id":"bMA4EFozR7Sz"},"source":["We then create a quantum device that will run our circuits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5F5nIthc-5t"},"outputs":[],"source":["dev = qml.device(\"default.qubit\", wires=4)\n","\n","@qml.qnode(dev)\n","def circuit(weights, x):\n","    qml.templates.AngleEmbedding(x, wires=range(4))\n","    qml.templates.StronglyEntanglingLayers(weights, wires=range(4))\n","    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1))"]},{"cell_type":"markdown","metadata":{"id":"nsa6nqSSDdU5"},"source":["Variational classifiers usually define a “layer” or “block”, which is an elementary circuit architecture that gets repeated to build the full variational circuit.\n","\n","Our circuit layer will use four qubits, or wires, and consists of an arbitrary rotation on every qubit, as well as a ring of CNOTs that entangles each qubit with its neighbor. Borrowing from machine learning, we call the parameters of the layer weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_q2jtSWbGWYG"},"outputs":[],"source":["def layer(layer_weights):\n","    for wire in range(4):\n","        qml.Rot(*layer_weights[wire], wires=wire)\n","\n","    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n","        qml.CNOT(wires)"]},{"cell_type":"markdown","metadata":{"id":"OAtoW-2lGPnc"},"source":["We also need a way to encode data inputs into the circuit, so that the measured output depends on the inputs. In this first example, the inputs are bitstrings, which we encode into the state of the qubits. The quantum state after state preparation is a computational basis state that has 1s where the input bitstring has 1s.\n","\n","The BasisState function provided by PennyLane is made to do just this. It expects the input to be a list of zeros and ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICyZHHdvGtlH"},"outputs":[],"source":["def state_preparation(x):\n","    qml.BasisState(x, wires=[0, 1, 2, 3])"]},{"cell_type":"markdown","metadata":{"id":"MTAOvz_3GPjm"},"source":[" we define the variational quantum circuit as this state preparation routine, followed by a repetition of the layer structure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSbFk0KjG5IO"},"outputs":[],"source":["@qml.qnode(dev)\n","def circuit(weights, x):\n","    state_preparation(x)\n","\n","    for layer_weights in weights:\n","        layer(layer_weights)\n","\n","    return qml.expval(qml.PauliZ(0))"]},{"cell_type":"markdown","metadata":{"id":"RB0Ze0KzRKq9"},"source":["If we want to add a “classical” bias parameter, the variational quantum classifier also needs some post-processing. We define the full model as a sum of the output of the quantum circuit, plus the trainable bias."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xXNG5X7RP2M"},"outputs":[],"source":["def variational_classifier(weights, bias, x):\n","    return circuit(weights, x) + bias"]},{"cell_type":"markdown","metadata":{"id":"BneXAN95RoqT"},"source":["## The cost function"]},{"cell_type":"markdown","metadata":{"id":"qrQl2zikGPfH"},"source":["\n","\n","In supervised learning, the cost function typically combines a loss function with a regularization term. For simplicity, we focus on the standard squared loss function, which evaluates how far the model's predictions are from the true target labels.\n","\n","Additionally, to evaluate the performance of the classifier, we use accuracy, which measures the percentage of predictions that match the true target labels. Accuracy provides a straightforward metric for assessing how often the classifier correctly identifies the correct class.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bG9Y-KnCd7X6"},"source":["First we initialize the variables.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tSTsa0HSeGp"},"outputs":[],"source":["def square_loss(labels, predictions):\n","    # We use a call to qml.math.stack to allow subtracting the arrays directly\n","    return np.mean((labels - qml.math.stack(predictions)) ** 2)"]},{"cell_type":"markdown","metadata":{"id":"SHH7q4c1SllV"},"source":["To monitor how many inputs the current classifier predicted correctly, we also define the accuracy, or the proportion of predictions that agree with a set of target labels.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDe42tWzSm-7"},"outputs":[],"source":["def accuracy(labels, predictions):\n","    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n","    acc = acc / len(labels)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"il-6bC_rTEzV"},"source":["During the training of our model, the cost function's computation hinges on the specific data used—namely, the features and labels that are processed in each optimization iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W93rnlFzTEPy"},"outputs":[],"source":["def cost(weights, bias, X, Y):\n","    predictions = [variational_classifier(weights, bias, x) for x in X]\n","    return square_loss(Y, predictions)"]},{"cell_type":"markdown","metadata":{"id":"xry1AdmWTr9V"},"source":["##Optimization"]},{"cell_type":"markdown","metadata":{"id":"csqqqLOHT3PO"},"source":["You need to load and preprocess some data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPiha_KpgDUh"},"outputs":[],"source":["def state_preparation(x):\n","    qml.BasisState(x, wires=[0, 1, 2, 3])"]},{"cell_type":"markdown","source":[" Loading the Iris Dataset and Preparing the Data"],"metadata":{"id":"2ToS2aaNoFkj"}},{"cell_type":"markdown","metadata":{"id":"h2Y2kGEKUpfg"},"source":["We initialize the variables randomly but fix a seed for reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1723008750623,"user":{"displayName":"menna zaied","userId":"13321966629572407703"},"user_tz":-180},"id":"5DJ0eNUhUsWh","outputId":"29017fd0-f777-4f04-cdc6-5adc006dbc1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Weights: [[[ 0.01764052  0.00400157  0.00978738]\n","  [ 0.02240893  0.01867558 -0.00977278]\n","  [ 0.00950088 -0.00151357 -0.00103219]\n","  [ 0.00410599  0.00144044  0.01454274]]\n","\n"," [[ 0.00761038  0.00121675  0.00443863]\n","  [ 0.00333674  0.01494079 -0.00205158]\n","  [ 0.00313068 -0.00854096 -0.0255299 ]\n","  [ 0.00653619  0.00864436 -0.00742165]]]\n","Bias:  0.0\n"]}],"source":["np.random.seed(0)\n","num_qubits = 4\n","num_layers = 2\n","weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n","bias_init = np.array(0.0, requires_grad=True)\n","\n","print(\"Weights:\", weights_init)\n","print(\"Bias: \", bias_init)"]},{"cell_type":"markdown","metadata":{"id":"WDCHHYfhU3LK"},"source":["Then  we  need to create an optimizer instance and choose a batch size…"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wjrOY62U8-v"},"outputs":[],"source":["opt = NesterovMomentumOptimizer(0.5)\n","batch_size = 5"]},{"cell_type":"markdown","metadata":{"id":"64h52TgVVARM"},"source":["Run the optimizer to train the model"]},{"cell_type":"code","source":["weights = weights_init\n","bias = bias_init\n","for it in range(100):\n","\n","    # Update the weights by one optimizer step, using only a limited batch of data\n","    batch_index = np.random.randint(0, len(X), (batch_size,))\n","    X_batch = X[batch_index]\n","    Y_batch = Y[batch_index]\n","    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n","\n","    # Compute accuracy\n","    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n","\n","    current_cost = cost(weights, bias, X, Y)\n","    acc = accuracy(Y, predictions)\n","\n","    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"],"metadata":{"id":"tqgHhLWkhB2_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With this in mind, let's evaluate the performance of our classifier on a separate test set of examples that were not used during training. This will give us a better understanding of how well our model can generalize to new, unseen data."],"metadata":{"id":"sGjvuAilhHKB"}},{"cell_type":"code","source":["data = np.loadtxt(\"variational_classifier/data/parity_test.txt\", dtype=int)\n","X_test = np.array(data[:, :-1])\n","Y_test = np.array(data[:, -1])\n","Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n","\n","predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n","\n","for x,y,p in zip(X_test, Y_test, predictions_test):\n","    print(f\"x = {x}, y = {y}, pred={p}\")\n","\n","acc_test = accuracy(Y_test, predictions_test)\n","print(\"Accuracy on unseen data:\", acc_test)"],"metadata":{"id":"dnyiGmqdhgM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2. Iris classification"],"metadata":{"id":"3vGUR-rBh1vi"}},{"cell_type":"markdown","source":["We're now going to tackle a more complex classification task using the Iris dataset, where each data point is represented as a 2-dimensional real-valued vector. To prepare this data for our quantum classifier, we'll add some \"latent dimensions\" to the vectors, allowing us to encode the inputs into a 2-qubit quantum state. This will enable us to explore the capabilities of our variational quantum classifier on a more realistic dataset"],"metadata":{"id":"Vfkj9i5PiTxM"}},{"cell_type":"markdown","source":["##Quantum and classical nodes"],"metadata":{"id":"wMVlgxz9iqcu"}},{"cell_type":"markdown","source":["**State Preparation for Real-Valued Vectors**"],"metadata":{"id":"hPwLya5mjDfA"}},{"cell_type":"markdown","source":["When working with real-valued vectors, state preparation becomes more intricate compared to representing bitstrings with basis states. Each input x needs to be translated into a set of angles, which are then fed into a small routine for state preparation. To simplify this process, we'll focus on data from the positive subspace, allowing us to ignore signs and avoid additional rotations around the Z-axis."],"metadata":{"id":"nGxHWA_6jNST"}},{"cell_type":"markdown","source":["**Circuit Implementation**"],"metadata":{"id":"pv7mNRW_jdTn"}},{"cell_type":"markdown","source":["\n","Our circuit is implemented based on the scheme outlined in Möttönen et al. (2004) and Schuld and Petruccione (2018), which is adapted for positive vectors only. Additionally, we've decomposed controlled Y-axis rotations into more basic gates, following the approach described in Nielsen and Chuang (2010).\n","\n","Let me know if you'd like me to summarize this as well!\n","\n","\n","\n","\n","Share\n","New Chat\n","Scroll to bottom\n","\n"],"metadata":{"id":"0qZ6oXjZjaLM"}},{"cell_type":"code","source":["def get_angles(x):\n","    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n","    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n","    beta2 = 2 * np.arcsin(np.linalg.norm(x[2:]) / np.linalg.norm(x))\n","\n","    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n","\n","\n","def state_preparation(a):\n","    qml.RY(a[0], wires=0)\n","\n","    qml.CNOT(wires=[0, 1])\n","    qml.RY(a[1], wires=1)\n","    qml.CNOT(wires=[0, 1])\n","    qml.RY(a[2], wires=1)\n","\n","    qml.PauliX(wires=0)\n","    qml.CNOT(wires=[0, 1])\n","    qml.RY(a[3], wires=1)\n","    qml.CNOT(wires=[0, 1])\n","    qml.RY(a[4], wires=1)\n","    qml.PauliX(wires=0)"],"metadata":{"id":"ygwf3xkJjhvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we could test this code by adding"],"metadata":{"id":"cwJoAYC2jnns"}},{"cell_type":"code","source":["x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\n","ang = get_angles(x)\n","\n","\n","@qml.qnode(dev)\n","def test(angles):\n","    state_preparation(angles)\n","\n","    return qml.state()\n","\n","\n","state = test(ang)\n","\n","print(\"x               : \", np.round(x, 6))\n","print(\"angles          : \", np.round(ang, 6))\n","print(\"amplitude vector: \", np.round(np.real(state), 6))"],"metadata":{"id":"IgImkLC6jsul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we're working with 2-qubit circuits, we need to modify the layer function to accommodate this change. This update will ensure that our variational quantum circuit is tailored to the new 2-qubit architecture."],"metadata":{"id":"OVmZf696j5EQ"}},{"cell_type":"code","source":["def layer(layer_weights):\n","    for wire in range(2):\n","        qml.Rot(*layer_weights[wire], wires=wire)\n","    qml.CNOT(wires=[0, 1])\n","\n","\n","def cost(weights, bias, X, Y):\n","    # Transpose the batch of input data in order to make the indexing\n","    # in state_preparation work\n","    predictions = variational_classifier(weights, bias, X.T)\n","    return square_loss(Y, predictions)"],"metadata":{"id":"_tQ5wfEJkDaN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##The data"],"metadata":{"id":"DuY_eP-MkJH0"}},{"cell_type":"markdown","source":["**Preparing the Iris Dataset**"],"metadata":{"id":"XgFFCvCYk1YS"}},{"cell_type":"markdown","source":["\n","We load the Iris dataset and preprocess it for our quantum classifier. This involves:\n","\n","Adding two \"latent dimensions\" to each data point to match the quantum state vector size\n","Normalizing the data to prevent feature dominance\n","Converting inputs to rotation angles using the get_angles function"],"metadata":{"id":"H9aV80-vkOH8"}},{"cell_type":"markdown","source":["**Remember**\n","\n","Download the Iris dataset and place it in the variational_classifer/data subfolder before proceeding."],"metadata":{"id":"Tx8bi8-rk5GC"}},{"cell_type":"code","source":["data = np.loadtxt(\"variational_classifier/data/iris_classes1and2_scaled.txt\")\n","X = data[:, 0:2]\n","print(f\"First X sample (original)  : {X[0]}\")\n","\n","# pad the vectors to size 2^2=4 with constant values\n","padding = np.ones((len(X), 2)) * 0.1\n","X_pad = np.c_[X, padding]\n","print(f\"First X sample (padded)    : {X_pad[0]}\")\n","\n","# normalize each input\n","normalization = np.sqrt(np.sum(X_pad**2, -1))\n","X_norm = (X_pad.T / normalization).T\n","print(f\"First X sample (normalized): {X_norm[0]}\")\n","\n","# the angles for state preparation are the features\n","features = np.array([get_angles(x) for x in X_norm], requires_grad=False)\n","print(f\"First features sample      : {features[0]}\")\n","\n","Y = data[:, -1]"],"metadata":{"id":"1fLT_CjUlDOb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've transformed the original data into a new set of features, represented as angles. That's why we've renamed the input data X to features. Now, let's visualize the preprocessing steps and experiment with different dimension combinations (like dim1 and dim2). We'll see that some of these new features still effectively distinguish between classes, while others are less useful for classification."],"metadata":{"id":"lquJSeCzmP8-"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure()\n","plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n","plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n","plt.title(\"Original data\")\n","plt.show()\n","\n","plt.figure()\n","dim1 = 0\n","dim2 = 1\n","plt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n","plt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n","plt.title(f\"Padded and normalised data (dims {dim1} and {dim2})\")\n","plt.show()\n","\n","plt.figure()\n","dim1 = 0\n","dim2 = 3\n","plt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n","plt.scatter(features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n","plt.title(f\"Feature vectors (dims {dim1} and {dim2})\")\n","plt.show()"],"metadata":{"id":"rkaXAWt-jMMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we want to take our model to the next level by enabling it to make predictions on unseen data. To achieve this, we'll train our model on one dataset and evaluate its performance on a separate dataset that it hasn't seen before. This is known as generalization. To keep track of how well our model generalizes, we'll divide our data into two subsets: a training set for model training and a validation set for performance monitoring."],"metadata":{"id":"AkI-2MGrmmU3"}},{"cell_type":"code","source":["np.random.seed(0)\n","num_data = len(Y)\n","num_train = int(0.75 * num_data)\n","index = np.random.permutation(range(num_data))\n","feats_train = features[index[:num_train]]\n","Y_train = Y[index[:num_train]]\n","feats_val = features[index[num_train:]]\n","Y_val = Y[index[num_train:]]\n","\n","# We need these later for plotting\n","X_train = X[index[:num_train]]\n","X_val = X[index[num_train:]]"],"metadata":{"id":"ExIIuRbUm0Ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Optimization**\n","First we initialize the variables."],"metadata":{"id":"1C9XdsLIm9g3"}},{"cell_type":"code","source":["opt = NesterovMomentumOptimizer(0.01)\n","batch_size = 5\n","\n","# train the variational classifier\n","weights = weights_init\n","bias = bias_init\n","for it in range(60):\n","    # Update the weights by one optimizer step\n","    batch_index = np.random.randint(0, num_train, (batch_size,))\n","    feats_train_batch = feats_train[batch_index]\n","    Y_train_batch = Y_train[batch_index]\n","    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n","\n","    # Compute predictions on train and validation set\n","    predictions_train = np.sign(variational_classifier(weights, bias, feats_train.T))\n","    predictions_val = np.sign(variational_classifier(weights, bias, feats_val.T))\n","\n","    # Compute accuracy on train and validation set\n","    acc_train = accuracy(Y_train, predictions_train)\n","    acc_val = accuracy(Y_val, predictions_val)\n","\n","    if (it + 1) % 2 == 0:\n","        _cost = cost(weights, bias, features, Y)\n","        print(\n","            f\"Iter: {it + 1:5d} | Cost: {_cost:0.7f} | \"\n","            f\"Acc train: {acc_train:0.7f} | Acc validation: {acc_val:0.7f}\"\n","        )"],"metadata":{"id":"t-lgQxBqnCAn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can plot the continuous output of the variational classifier for the first two dimensions of the Iris data set."],"metadata":{"id":"Pu0Kd0jwnHET"}},{"cell_type":"code","source":["plt.figure()\n","cm = plt.cm.RdBu\n","\n","# make data for decision regions\n","xx, yy = np.meshgrid(np.linspace(0.0, 1.5, 30), np.linspace(0.0, 1.5, 30))\n","X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n","\n","# preprocess grid points like data inputs above\n","padding = 0.1 * np.ones((len(X_grid), 2))\n","X_grid = np.c_[X_grid, padding]  # pad each input\n","normalization = np.sqrt(np.sum(X_grid**2, -1))\n","X_grid = (X_grid.T / normalization).T  # normalize each input\n","features_grid = np.array([get_angles(x) for x in X_grid])  # angles are new features\n","predictions_grid = variational_classifier(weights, bias, features_grid.T)\n","Z = np.reshape(predictions_grid, xx.shape)\n","\n","# plot decision regions\n","levels = np.arange(-1, 1.1, 0.1)\n","cnt = plt.contourf(xx, yy, Z, levels=levels, cmap=cm, alpha=0.8, extend=\"both\")\n","plt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\n","plt.colorbar(cnt, ticks=[-1, 0, 1])\n","\n","# plot data\n","for color, label in zip([\"b\", \"r\"], [1, -1]):\n","    plot_x = X_train[:, 0][Y_train == label]\n","    plot_y = X_train[:, 1][Y_train == label]\n","    plt.scatter(plot_x, plot_y, c=color, marker=\"o\", ec=\"k\", label=f\"class {label} train\")\n","    plot_x = (X_val[:, 0][Y_val == label],)\n","    plot_y = (X_val[:, 1][Y_val == label],)\n","    plt.scatter(plot_x, plot_y, c=color, marker=\"^\", ec=\"k\", label=f\"class {label} validation\")\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ZbdWXYwfnn6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnmIRX_PehLo"},"source":["picture for the output graf"]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1715182909637}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}